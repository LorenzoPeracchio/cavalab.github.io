---
title: "Translating Intersectionality to Fair Machine Learning"
date: 2023-05-05
layout: posts
author: bill
excerpt: A new perspective on how this social theory relates to fair machine learning.
tags: 
    - Machine Learning
    - Fairness
header:
    teaser: /assets/images/clique.png
---
{:.notice--info}
**Further Reading**
{% include citation.html pubid="2023LettTranslatingintersectionalityto" %}

[^paper]: Lett, E., & La Cava, W. G. (2023).  Translating intersectionality to fair machine learning in health sciences.  *Nature Machine Intelligence*. [doi](https://doi.org/10.1038/s42256-023-00651-3), [preprint (PDF)](https://osf.io/download/63fd1bd940cecd079876f20c/)
[^collins]: Collins, P. H. & Bilge, S. Intersectionality (John Wiley & Sons, 2020). 

The field of fair machine learning has sought to make many social concepts as concrete as possible, so that they can be reflected in the socio-technical artifacts we are building in the world. In a new commentary led by [Dr. Elle Lett](https://cavalab.org/members/lett-elle/), we argue that the social theory of *intersectionality* has more to offer fair ML beyond the reporting of model metrics over intersectional subgroups[^paper]. 

In this piece, we review the six concepts for intersectionality theory articulated by Collins and Bilge in their textbook, *Intersectionality*[^collins]. These core concepts pertain to many parts of the ML pipeline beyond model training, including data collection/generation, interpretability, transportability between sites, and post-deployment impact studies.  

Intersectionality is not a problem to be solved. 
Instead, as a critical theory, it prompts us to confront difficult questions.
For example: how do we address fairness implications for minoritized groups when our measurement certainty is limited by sample size? 
Furthermore, it suggests discretion in deploying ML technologies: some use cases may not be appropriate for ML if data cannot sufficiently represent marginalized groups or tools cannot be fairly deployed. 